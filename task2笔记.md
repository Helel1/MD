# task2笔记

## **1. regression： [回归](https://baike.baidu.com/item/回归/10412815?fr=aladdin)**

> 回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。

应用举例：

- 预测宝可梦进化后的CP值

  - 输入：进化前的CP值
  - 输出：进化后的CP值
- 股市预测
  - 输入：近十年的股票变动、新闻资讯等
  - 输出：预测明天股市的平均值
- 自动驾驶
  - 输入：车上每个sensor的数据，例如路况、车距等
  - 输出：方向盘的角度
- 商品推荐
  - 输入：商品a的特性、商品b的特性
  - 输出：购买商品的可能性

## **2. 建立模型**（以宝可梦进化为例）

### Step1：模型假设-线性模型

#### 根据具体情况可以选择**一元线性模型**或多**元线性模型**

- 一元线性模型

  
  $$
  以一个特征值x_{cp}为例，能建立
  y=b+wx_{cp}
  一元线性模型
  $$
  其中w，b是可以任意取的，但是得符合实际，比如在此实例中进化后的CP不可能减少，所以
  $$
  w>0
  $$
  
- 多元线性模型

  在这个例子中，实际的特征值肯定不止一个，还有，进化前的CP值，物种，血量等。所以我们建立以下线性模型
  $$
  y = b + \sum w_ix_i： (x_i为各种特征值，w_i为各个特征值的权重，b为偏移量)
  $$
  

### Step2：模型评估-损失函数(单个特征值)

![](E:\桌面\截图\Snipaste_2021-07-14_22-55-11.png)

从数学的角度，**预测的CP值**和**真实的CP值**的差可以用他们的距离表示，于是我们定义损失函数Loss function：
$$
L(w,b) = \sum_{n=1}^{10}(y^{’}-(b+w·x_{cp}^n))^2
$$

$$
其中y^{’}为真是进化后的cp值，(b+w·x_{cp}^n)为预测进化后的cp值，x_{cp}^n为进化前的cp
$$

### Step3：最佳模型-梯度下降

对于我们定义的损失函数，要找到最佳模型就是找到合适的w和b使得L取最小值。

#### 首先考虑一个参数w，

- 先随机取一个
  $$
  w^0
  $$
  
- 计算微分，也就是斜率，然后根据斜率移动w

  - 斜率大于0向左移动（减小w）
  - 斜率小于0向右移动（增大w）

- 根据学习率移动
  $$
  学习率为\eta，可以根据自己需要调整
  $$
  
- 重复以上两步，直到找到最低点

  过程如图所示：

  ![](E:\桌面\截图\Snipaste_2021-07-14_23-21-06.png)

  ![](E:\桌面\截图\Snipaste_2021-07-14_23-22-23.png)

#### 接着考虑两个参数w和b

两个参数的过程和上面一个参数一样，唯一不一样的是求导过程为求偏导、

如图所示：

![](E:\桌面\截图\Snipaste_2021-07-14_23-23-00.png)

- 每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小
- 红色的箭头代表等高线的法线方向

**这样我们应该就能得到当前最优解，但是很显然这个不是最终的最优解，接下来进行优化。**

## 3.**优化模型**

我们开始按步骤优化

### 首先Step1

通过输入更多宝可梦数据，我们发现相同的起始CP值，但进化后的CP差距竟然是2倍（如图）。显然，不同种类的宝可梦，进化后CP值变化幅度不同，因此可以给不同中类的宝可梦建立不同的模型，然后再通过种类判断将四个模型，合并到一个模型中。

![](E:\桌面\截图\Snipaste_2021-07-14_23-41-48.png)



```python
x1,x2,x3,x4=0, 0, 0, 0

if x ==  x1:
	x1 = 1
	y = x1(b1+w1*x) + x2(b2+w2*x) + x3(b3+w3*x) + x4(b4+w4*x)
if x ==  x2:
	x2 = 1
	y = x1(b1+w1*x) + x2(b2+w2*x) + x3(b3+w3*x) + x4(b4+w4*x)    
if x ==  x3:
	x3 = 1
	y = x1(b1+w1*x) + x2(b2+w2*x) + x3(b3+w3*x) + x4(b4+w4*x)
if x ==  x4:
	x4 = 1
	y = x1(b1+w1*x) + x2(b2+w2*x) + x3(b3+w3*x) + x4(b4+w4*x)
```

优化后：

![](E:\桌面\截图\Snipaste_2021-07-14_23-59-04.png)

### Step2优化

将血量（HP）、重量（Weight）、高度（Height）也加入到模型中。

![](E:\桌面\截图\Snipaste_2021-07-15_00-06-06.png)

从训练结果来看，训练误差很小，看起来像是很成功了，但是一旦测试就发现误差很大，我们把这种现象称为[过拟合（overfitting)](https://baike.baidu.com/item/过拟合/3359778?fr=aladdin)

> 过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。

因此我们加入正则化。

### Step3优化：[正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96/5739561?fr=aladdin)

> 正则化(regularization)，是指在线性代数理论中，不适定问题通常是由一组线性代数方程定义的，而且这组方程组通常来源于有着很大的条件数的不适定反问题。大条件数意味着舍入误差或其它误差会严重地影响问题的结果。

正则化的目的就是减小因变量随自变量的变化而变化的幅度。

正则化之后：

![](E:\桌面\截图\Snipaste_2021-07-15_00-44-44.png)

